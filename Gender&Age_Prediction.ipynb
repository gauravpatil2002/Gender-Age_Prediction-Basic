{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTBfH1RjPJ_B",
        "outputId": "fe7e9121-265c-4b28-8d05-42f5acc28fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to your model files\n",
        "face_model_path = \"/content/opencv_face_detector_uint8.pb\"\n",
        "face_config_path = \"/content/opencv_face_detector.pbtxt\"\n",
        "age_model_path = \"/content/age_net.caffemodel\"\n",
        "age_config_path = \"/content/age_deploy.prototxt\"\n",
        "gender_model_path = \"/content/gender_net.caffemodel\"\n",
        "gender_config_path = \"/content/gender_deploy.prototxt\""
      ],
      "metadata": {
        "id": "9d7uQHsaUAot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "face_net = cv2.dnn.readNet(face_model_path, face_config_path)\n",
        "age_net = cv2.dnn.readNet(age_model_path, age_config_path)\n",
        "gender_net = cv2.dnn.readNet(gender_model_path, gender_config_path)"
      ],
      "metadata": {
        "id": "QNzSsrf9UCo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define age and gender lists\n",
        "age_list = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "gender_list = ['Male', 'Female']"
      ],
      "metadata": {
        "id": "BiyEJdXaUqUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_and_predict(image):\n",
        "    # Create a blob from the image\n",
        "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104, 117, 123))\n",
        "\n",
        "    # Detect faces\n",
        "    face_net.setInput(blob)\n",
        "    detections = face_net.forward()\n",
        "\n",
        "    # Process each detected face\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            # Get face box coordinates\n",
        "            box = detections[0, 0, i, 3:7] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
        "            (x, y, x1, y1) = box.astype(\"int\")\n",
        "\n",
        "            # Extract face ROI\n",
        "            face = image[y:y1, x:x1]\n",
        "\n",
        "            # Prepare face for age and gender prediction\n",
        "            face_blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
        "\n",
        "            # Predict gender\n",
        "            gender_net.setInput(face_blob)\n",
        "            gender_preds = gender_net.forward()\n",
        "            gender = gender_list[gender_preds[0].argmax()]\n",
        "\n",
        "            # Predict age\n",
        "            age_net.setInput(face_blob)\n",
        "            age_preds = age_net.forward()\n",
        "            age = age_list[age_preds[0].argmax()]\n",
        "\n",
        "            # Draw rectangle and label on the image\n",
        "            cv2.rectangle(image, (x, y), (x1, y1), (0, 255, 0), 2)\n",
        "            label = f\"{gender}, {age}\"\n",
        "            cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "mL8ribMaUtvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all images in the directory\n",
        "image_dir = \".\"  # Current directory\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            result = detect_and_predict(image)\n",
        "            cv2.imwrite(f\"output_{filename}\", result)\n",
        "            print(f\"Processed {filename}\")\n",
        "\n",
        "print(\"All images processed. Check the output files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnTLdmf9VIza",
        "outputId": "ae83e916-4e0f-4a70-a131-e70a3f95b4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed kid2.jpg\n",
            "Processed girl2.jpg\n",
            "Processed woman1.jpg\n",
            "Processed girl1.jpg\n",
            "Processed man1.jpg\n",
            "Processed man2.jpg\n",
            "Processed kid1.jpg\n",
            "All images processed. Check the output files.\n"
          ]
        }
      ]
    }
  ]
}